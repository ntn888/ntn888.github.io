<!doctype html><html lang=en><head><meta charset=utf-8><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content="light dark" name=color-scheme><title>Why run local LLMs?</title><link href=/img/favicon-32x32.png rel=icon sizes=32x32 type=image/png><link href=/img/favicon-16x16.png rel=icon sizes=16x16 type=image/png><link href=/img/apple-touch-icon.png rel=apple-touch-icon sizes=180x180><link href=https://fonts.googleapis.com rel=preconnect><link crossorigin href=https://fonts.gstatic.com rel=preconnect><link href="https://fonts.googleapis.com/css2?family=Signika&display=swap" rel=stylesheet><style>body{--primary-color:#698bcf;--primary-pale-color:#698bcf1c;--text-color:#3c4043;--text-pale-color:#9aa2b9;--bg-color:#fff;--highlight-mark-color:#5f75b045;--callout-note-color:#698bcf;--callout-important-color:#9971d9;--callout-warning-color:#c99054;--callout-alert-color:#d35757;--callout-question-color:#4985a2;--callout-tip-color:#3ea06f;--main-font:'Signika',ui-sans-serif,system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";--code-font:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;--homepage-max-width:750px;--main-max-width:750px;--avatar-size:70px;--paragraph-font-size:18px;--paragraph-line-height:1.75;--aside-font-size:16px;--img-border-radius:0;--inline-code-border-radius:2px}body.dark{--primary-color:#698bcf;--primary-pale-color:#698bcf1c;--text-color:#9197a5;--text-pale-color:#5d6470;--bg-color:#202124;--highlight-mark-color:#5f75b045;--callout-note-color:#698bcf;--callout-important-color:#9971d9;--callout-warning-color:#c99054;--callout-alert-color:#d35757;--callout-question-color:#4985a2;--callout-tip-color:#3ea06f}</style><link href=/main.css rel=stylesheet><link href=/hl-light.css id=hl rel=stylesheet><script data-domain=ntn888.github.io defer src=https://mypa.simplycreate.xyz/js/script.js></script><body class=post><script>if(localStorage.getItem('theme')=='dark'){document.body.classList.add('dark');const a=document.querySelector('link#hl');if(a)a.href='/hl-dark.css'}</script><header class=blur><div id=header-wrapper><nav><a href=/>ajit</a><span class=separator>::</span><a href=/blog>blog</a></nav><div id=btns><a aria-label="rss feed" href=/feed.xml><svg viewbox="0 0 24 24" height=24 width=24 xmlns=http://www.w3.org/2000/svg><path d="M3 17C5.20914 17 7 18.7909 7 21H3V17ZM3 10C9.07513 10 14 14.9249 14 21H12C12 16.0294 7.97056 12 3 12V10ZM3 3C12.9411 3 21 11.0589 21 21H19C19 12.1634 11.8366 5 3 5V3Z" fill=currentColor></path></svg></a><button aria-label="theme switch" data-moon-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24"><path d="M10 7C10 10.866 13.134 14 17 14C18.9584 14 20.729 13.1957 21.9995 11.8995C22 11.933 22 11.9665 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12C2 6.47715 6.47715 2 12 2C12.0335 2 12.067 2 12.1005 2.00049C10.8043 3.27098 10 5.04157 10 7ZM4 12C4 16.4183 7.58172 20 12 20C15.0583 20 17.7158 18.2839 19.062 15.7621C18.3945 15.9187 17.7035 16 17 16C12.0294 16 8 11.9706 8 7C8 6.29648 8.08133 5.60547 8.2379 4.938C5.71611 6.28423 4 8.9417 4 12Z" fill="currentColor"></path></svg>' data-sun-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24"><path d="M12 18C8.68629 18 6 15.3137 6 12C6 8.68629 8.68629 6 12 6C15.3137 6 18 8.68629 18 12C18 15.3137 15.3137 18 12 18ZM12 16C14.2091 16 16 14.2091 16 12C16 9.79086 14.2091 8 12 8C9.79086 8 8 9.79086 8 12C8 14.2091 9.79086 16 12 16ZM11 1H13V4H11V1ZM11 20H13V23H11V20ZM3.51472 4.92893L4.92893 3.51472L7.05025 5.63604L5.63604 7.05025L3.51472 4.92893ZM16.9497 18.364L18.364 16.9497L20.4853 19.0711L19.0711 20.4853L16.9497 18.364ZM19.0711 3.51472L20.4853 4.92893L18.364 7.05025L16.9497 5.63604L19.0711 3.51472ZM5.63604 16.9497L7.05025 18.364L4.92893 20.4853L3.51472 19.0711L5.63604 16.9497ZM23 11V13H20V11H23ZM4 11V13H1V11H4Z" fill="currentColor"></path></svg>' id=theme-toggle><svg viewbox="0 0 24 24" height=24 width=24 xmlns=http://www.w3.org/2000/svg><path d="M10 7C10 10.866 13.134 14 17 14C18.9584 14 20.729 13.1957 21.9995 11.8995C22 11.933 22 11.9665 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12C2 6.47715 6.47715 2 12 2C12.0335 2 12.067 2 12.1005 2.00049C10.8043 3.27098 10 5.04157 10 7ZM4 12C4 16.4183 7.58172 20 12 20C15.0583 20 17.7158 18.2839 19.062 15.7621C18.3945 15.9187 17.7035 16 17 16C12.0294 16 8 11.9706 8 7C8 6.29648 8.08133 5.60547 8.2379 4.938C5.71611 6.28423 4 8.9417 4 12Z" fill=currentColor></path></svg></button><button aria-label="table of content" id=toc-toggle><svg viewbox="0 0 24 24" height=24 width=24 xmlns=http://www.w3.org/2000/svg><path d="M3 4H21V6H3V4ZM3 11H15V13H3V11ZM3 18H21V20H3V18Z" fill=currentColor></path></svg></button></div></div></header><div id=wrapper><div id=blank></div><aside class=blur><nav><ul><li><a class=h2 href=#more-flexibility-and-control>More Flexibility and Control</a> <ul><li><a class=h3 href=#fine-tuning-unlocks-targeted-performance>Fine-Tuning Unlocks Targeted Performance</a><li><a class=h3 href=#optimizing-compute-for-variable-demand>Optimizing Compute for Variable Demand</a><li><a class=h3 href=#agility-to-update-experiment-and-improve>Agility to Update, Experiment and Improve</a></ul><li><a class=h2 href=#enhanced-privacy-and-security>Enhanced Privacy and Security</a> <ul><li><a class=h3 href=#sensitive-data-stays-onsite>Sensitive Data Stays Onsite</a><li><a class=h3 href=#streamlining-regulatory-and-policy-compliance>Streamlining Regulatory and Policy Compliance</a><li><a class=h3 href=#cutting-external-dependencies-improves-security-posture>Cutting External Dependencies Improves Security Posture</a></ul><li><a class=h2 href=#lower-long-term-costs>Lower Long-Term Costs</a> <ul><li><a class=h3 href=#avoiding-mounting-api-expenses>Avoiding Mounting API Expenses</a><li><a class=h3 href=#leveraging-existing-infrastructure>Leveraging Existing Infrastructure</a><li><a class=h3 href=#improved-return-as-models-compound-gains>Improved Return as Models Compound Gains</a></ul><li><a class=h2 href=#easier-integration-and-customization>Easier Integration and Customization</a> <ul><li><a class=h3 href=#streamlining-connections-to-internal-data-apps>Streamlining Connections to Internal Data & Apps</a><li><a class=h3 href=#building-custom-interfaces-experiences>Building Custom Interfaces & Experiences</a><li><a class=h3 href=#innovating-new-products-services>Innovating New Products & Services</a></ul><li><a class=h2 href=#conclusion>Conclusion</a></ul></nav><button aria-label="back to top" id=back-to-top><svg viewbox="0 0 24 24" height=24 width=24 xmlns=http://www.w3.org/2000/svg><path d="M11.9997 10.8284L7.04996 15.7782L5.63574 14.364L11.9997 8L18.3637 14.364L16.9495 15.7782L11.9997 10.8284Z" fill=currentColor></path></svg></button></aside><main><div><div data-check-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24"><path d="M10.0007 15.1709L19.1931 5.97852L20.6073 7.39273L10.0007 17.9993L3.63672 11.6354L5.05093 10.2212L10.0007 15.1709Z" fill="currentColor"></path></svg>' data-copy-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24"><path d="M6.9998 6V3C6.9998 2.44772 7.44752 2 7.9998 2H19.9998C20.5521 2 20.9998 2.44772 20.9998 3V17C20.9998 17.5523 20.5521 18 19.9998 18H16.9998V20.9991C16.9998 21.5519 16.5499 22 15.993 22H4.00666C3.45059 22 3 21.5554 3 20.9991L3.0026 7.00087C3.0027 6.44811 3.45264 6 4.00942 6H6.9998ZM5.00242 8L5.00019 20H14.9998V8H5.00242ZM8.9998 6H16.9998V16H18.9998V4H8.9998V6Z" fill="currentColor"></path></svg>' id=copy-cfg style=display:none></div><article data-backlink-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="20" height="20"><path d="M9.41421 8L18.0208 16.6066L16.6066 18.0208L8 9.41421V17H6V6H17V8H9.41421Z" fill="currentColor"></path></svg>' class=prose><h1>Why run local LLMs?</h1><div id=post-info><div id=date><span id=publish>2023-11-30</span></div><div id=tags><a href=https://ntn888.github.io//tags/opensource><span>#</span>opensource</a><a href=https://ntn888.github.io//tags/ai><span>#</span>AI</a><a href=https://ntn888.github.io//tags/selfhosted><span>#</span>selfhosted</a><a href=https://ntn888.github.io//tags/llamav2><span>#</span>llamav2</a></div></div><p>Have you ever wished your AI assistant was more flexible? More customized to your specific needs? More under your direct control? As remarkable as large language models like GPT-4 and Bard have been, relying solely on external APIs can leave developers feeling constrained. There's a better way – self-hosting your own private AI.<p>I distinctly remember the first time I used GPT-4. The sheer power and fluidity of its responses blew my mind. But after the initial wonder faded, limitations began to emerge. Requests for complex code would hit computation limits. Queries using sensitive data made me uneasy. And being dependent on a third-party API limited what customizations I could make.<p><img alt="Friendly chat bot in space" src=/img/friendly-chatbot.resized.png><p>Like Dorothy realizing her magical Oz was being powered by a man behind a curtain, shifting to a self-hosted large language model lifted the veil for me. Finally I had an AI assistant answering to me alone, trained on my data, living on my infrastructure. The flexibility, control, privacy, cost savings, and custom integrations unlocked have been a revelation.<p>As AI rapidly becomes essential business infrastructure, more teams are arriving at the same revelation. Self-hosted solutions allow you to mold a private AI assistant to your exact needs, without unpredictable costs or outside entanglements. Read on as we explore the benefits, options, and considerations shifting from rented to owned AI. I may be biased, but I believe you too will prefer having your friendly AI wizard in-house.<h2 id=more-flexibility-and-control>More Flexibility and Control<a aria-label="Anchor link for: more-flexibility-and-control" class=zola-anchor href=#more-flexibility-and-control>#</a></h2><p>When it comes to leveraging AI, flexibility and control are everything. Relying on rigid external APIs can leave your options limited. By self-hosting a large language model, you open new dimensions of customization to tailor it precisely to your goals.<h3 id=fine-tuning-unlocks-targeted-performance>Fine-Tuning Unlocks Targeted Performance<a aria-label="Anchor link for: fine-tuning-unlocks-targeted-performance" class=zola-anchor href=#fine-tuning-unlocks-targeted-performance>#</a></h3><p>Pre-trained models like GPT-4 and Bard display remarkable breadth, but their general knowledge comes at the cost of precision on specialized tasks. Fine-tuning allows you to customize models by continuing to train them on your own data, steering their intelligence toward your specific needs.<p>Whether improving code suggestions, analyzing scientific papers, or answering domain-specific questions, additional training refines their relevancy and accuracy. On-premise models give you the power over this enhancement process instead of being limited by a third party. The more tailored to your use cases, the better it performs.<h3 id=optimizing-compute-for-variable-demand>Optimizing Compute for Variable Demand<a aria-label="Anchor link for: optimizing-compute-for-variable-demand" class=zola-anchor href=#optimizing-compute-for-variable-demand>#</a></h3><p>Self-hosted AI allows you to scale resources fluidly to meet fluctuating needs. Server capacity can expand seamlessly via the cloud to handle spikes in model queries. When demand decreases, reduce resources accordingly. This saves substantially versus paying for peak API capacity at all times.<p>The ability to load balance queries across devices and data centers further bolsters flexible distribution of compute. You also retain control to upgrade to new hardware quickly instead of relying on a provider’s refresh cycle, keeping model performance on the cutting edge.<h3 id=agility-to-update-experiment-and-improve>Agility to Update, Experiment and Improve<a aria-label="Anchor link for: agility-to-update-experiment-and-improve" class=zola-anchor href=#agility-to-update-experiment-and-improve>#</a></h3><p>On-premise models empower your team to rapidly iterate experiments and innovations in AI capabilities. Changes don’t require coordination across organizations or account managers. Want to try enhancing prompts or expanding the training dataset? On your own systems, new ideas can be tested straightaway.<p>Having your AI engine in-house frees you to build additional tools and custom interfaces tailored to your workflows. As new techniques and model architectures emerge, self-hosted AI means you dictate the integration pace rather than being locked into a rigid vendor roadmap. With great power over your AI comes great product responsibility – are you ready?<h2 id=enhanced-privacy-and-security>Enhanced Privacy and Security<a aria-label="Anchor link for: enhanced-privacy-and-security" class=zola-anchor href=#enhanced-privacy-and-security>#</a></h2><p>When relying on external AI systems, privacy and security considerations can quickly spiral into headaches. By retaining direct control over data and models on internal infrastructure, self-hosted LLMs simplify safeguarding information while meeting compliance needs.<h3 id=sensitive-data-stays-onsite>Sensitive Data Stays Onsite<a aria-label="Anchor link for: sensitive-data-stays-onsite" class=zola-anchor href=#sensitive-data-stays-onsite>#</a></h3><p>Every industry deals with confidential data, whether customer info, financials, medical history or IP. Transmitting this data externally to utilize AI APIs rightly raises alarms for security and compliance teams. Just because insights from the data have business value doesn't mean IT oversight should be bypassed.<p>Self-hosted AI eliminates these concerns by keeping all processing onsite. Data remains within your firewall at all times, visible only to approved internal teams. With proper access controls, even admins running the AI systems can be prevented from directly viewing sensitive information used to train models.<h3 id=streamlining-regulatory-and-policy-compliance>Streamlining Regulatory and Policy Compliance<a aria-label="Anchor link for: streamlining-regulatory-and-policy-compliance" class=zola-anchor href=#streamlining-regulatory-and-policy-compliance>#</a></h3><p>From financial regulations like GDPR to healthcare rules like HIPAA, evolving legal expectations make safe data handling trickier by the year. By retaining data and AI systems in-house instead of relying on cloud services, the barriers to compliance are dramatically reduced.<p>Your own infrastructure allows auditing and controls tailored to your exact regulatory needs, with less dependence on third-party attestations. Data residency laws also come into play requiring information stay within national borders, easily addressed via on-premise solutions.<h3 id=cutting-external-dependencies-improves-security-posture>Cutting External Dependencies Improves Security Posture<a aria-label="Anchor link for: cutting-external-dependencies-improves-security-posture" class=zola-anchor href=#cutting-external-dependencies-improves-security-posture>#</a></h3><p>Every external API call or cloud service dependency increases vulnerabilities by expanding the corporate attack surface. Just look at the barrage of stabilizer AI incidents last year! Self-hosted models help prevent such headaches by eliminating external connections associated with AI functions.<p>Isolation also enables creating something like an "air gap" via machinery only used for model handling, disconnected from wider business networks. Though not bulletproof, minimizing touch points via private AI infrastructure pushes security in the right direction.<h2 id=lower-long-term-costs>Lower Long-Term Costs<a aria-label="Anchor link for: lower-long-term-costs" class=zola-anchor href=#lower-long-term-costs>#</a></h2><p>When evaluating the financial implications of AI systems, it's essential to take a big picture perspective. Though recognizing ongoing costs, over years self-hosted solutions often prove far more economical than reliance on rental APIs requiring endless subscription fees.<h3 id=avoiding-mounting-api-expenses>Avoiding Mounting API Expenses<a aria-label="Anchor link for: avoiding-mounting-api-expenses" class=zola-anchor href=#avoiding-mounting-api-expenses>#</a></h3><p>It's easy to only compare upfront expenses when adopting new technology, failing to account for recurring fees endless draining budgets over time. Leading LLM APIs often run $0.002+ per 1,000 tokens processed. For context, this essay already tallied over 3,400 tokens – costing over $6 at that rate!<p>While the convenience of instantly available AI is appealing, even moderate enterprise usage adds up to staggering sums. Budget-conscious leaders rightly question chiefly benefiting API shareholder value long-term for functionality becoming a commodity.<h3 id=leveraging-existing-infrastructure>Leveraging Existing Infrastructure<a aria-label="Anchor link for: leveraging-existing-infrastructure" class=zola-anchor href=#leveraging-existing-infrastructure>#</a></h3><p>Rather than building from scratch, self-hosted AI can integrate with current on-premise servers and hardware many enterprises already own. Though still representing an investment, extending existing capacity is far cheaper than standalone rental expenses.<p>Private AI also allows you to dictate upgrade cycles rather than relying on a provider's hardware refresh rate. Regular advances in GPU/TPU processing mean efficiency gains offsetting growing model sizes. In five years the compute powering innovations today could easily fit on a desktop.<h3 id=improved-return-as-models-compound-gains>Improved Return as Models Compound Gains<a aria-label="Anchor link for: improved-return-as-models-compound-gains" class=zola-anchor href=#improved-return-as-models-compound-gains>#</a></h3><p>A core advantage of LLMs lies in their ability to build upon prior learning. Over months and years of consistent data exposure, even hosted locally their performances continue improving. This means models become an appreciating asset intrinsically delivering multiplying value beyond static rental APIs.<p>With customer interactions, new products, and technical advances expanding data pools,Compose even longer form content with deeper analysis the accuracy and quality of outputs compound faster on privately controlled infrastructure. Much like wine aging to perfection in your cellar.<h2 id=easier-integration-and-customization>Easier Integration and Customization<a aria-label="Anchor link for: easier-integration-and-customization" class=zola-anchor href=#easier-integration-and-customization>#</a></h2><p>Beyond core model functionality, realizing AI's full potential requires tailored integration with business systems and processes. Self-hosted infrastructure fosters frictionless customization that rented APIs simply can't match.<h3 id=streamlining-connections-to-internal-data-apps>Streamlining Connections to Internal Data & Apps<a aria-label="Anchor link for: streamlining-connections-to-internal-data-apps" class=zola-anchor href=#streamlining-connections-to-internal-data-apps>#</a></h3><p>Extracting maximum value from AI necessitates easy interoperability with other stacks powering operations. Self-hosted models co-located on-premise simplify linking to internal databases, analytics tools, CRM and ERP platforms etc. without external touchpoints.<p>With direct data access, models can programmatically pull the latest info and update training without manual efforts. Code can connect to other apps via API allowing AI to enhance workflows across departments. Avoid integration hassles or changes breaking links to external providers.<h3 id=building-custom-interfaces-experiences>Building Custom Interfaces & Experiences<a aria-label="Anchor link for: building-custom-interfaces-experiences" class=zola-anchor href=#building-custom-interfaces-experiences>#</a></h3><p>The client interface heavily impacts perceived AI quality by employees and customers. Rented APIs mean being stuck with vanilla experiences, but self-hosted options empower developing bespoke tools aligned to your brand.<p>Beyond skins and themes, you can tailor interactions to specific audiences within your organization. Data scientists may prefer Python notebooks while business analysts appreciate no-code web UIs. White label the output for customer facing applications. The sky's limit when controlling both model and interface.<h3 id=innovating-new-products-services>Innovating New Products & Services<a aria-label="Anchor link for: innovating-new-products-services" class=zola-anchor href=#innovating-new-products-services>#</a></h3><p>Owning the full AI stack fosters launching entirely new solutions. As examples, AI could personalized customer marketing content, analyze warranty claims and highlight areas for engineering improvements, or bootstrap insurance policy document review.<p>With the flexibility to evolve models and build around their capabilities, you're only limited by imagination rather than restrictions imposed by external platforms. Build a strategic advantage by doubling down on proprietary efforts rivals can’t replicate relying on vendors.<h2 id=conclusion>Conclusion<a aria-label="Anchor link for: conclusion" class=zola-anchor href=#conclusion>#</a></h2><p>As we've explored across critical areas like flexibility, security, costs, and customization, self-hosting your own large language model for private use provides transformational advantages compared to reliance on external AI rental services.<p>By retaining direct control over your AI assistant within your own infrastructure, you gain unmatched ability to customize to your specific data, workflows, and evolving needs. Keeping processing on-premise together with the sensitive information used for training also slashes compliance risks and data privacy concerns.<p>And while upfront investment is required, over the long-term self-hosted models can significantly reduce expenses versus open-ended API subscriptions. All while better leverage of existing systems and multiplying accuracy through continual learning compound benefits.<p>I encourage you to seriously pursue bringing customized AI capabilities in-house. Start small if needed (See <a href=https://ntn888.github.io/blog/llama-howto/>this post</a> for inspiration!), but the long-term dividends across security, costs, and performance make owning your AI absolutely worthwhile. Just be careful as it can become highly addictive once tuned to your goals!<p>The essential next step is evaluating options matching deployment and training requirements, but with the right vision the loops of constant improvement can truly make a private AI assistant feel like your devoted partner in innovation. Here's to a more customized, controlled AI future.</article></div><footer><div class=copyright><p>© 2023 Ajit</div><div class=credits>powered by <a rel="noreferrer noopener" href=https://www.getzola.org target=_blank>zola</a> and <a rel="noreferrer noopener" href=https://github.com/isunjn/serene target=_blank>serene</a></div></footer></main></div><script src=/js/lightense.min.js></script><script src=/js/main.js></script>